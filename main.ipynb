{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run LanguageModel.py\n",
    "%run DataLoader.py\n",
    "%run encoder.py\n",
    "%run decoder.py\n",
    "%run seq2seq.py\n",
    "%run model_config.py\n",
    "%run metrics.py\n",
    "%run ScorePrinter.py\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader('train', ('de', 'en'), max_length = 50, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = DataLoader('dev', ('de', 'en'), languageModels = train_dl.languageModels, max_length = 50, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1 = train_dl.languageModels[train_dl.languages[0]]\n",
    "lm2 = train_dl.languageModels[train_dl.languages[1]]\n",
    "model_config = ModelConfig(input_size = lm1.n_tokens, beam_width = 3, hidden_size = 256, output_size = lm2.n_tokens, rnn_type='GRU', bidirectional=False, attention = 'local', max_length=52)\n",
    "#checkpoint = torch.load(\"./state_dict.tar\")\n",
    "s2s = seq2seq(model_config=model_config, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(epochs, print_every=1000):\n",
    "    n_iters = len(train_dl)\n",
    "    score_printer = ScorePrinter(\"Training\", [('NLL', loss_metric),('Perplexity', perplexity)])\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        score_printer.startEpoch(epoch)\n",
    "        idx_permutation = np.random.permutation(len(train_dl))\n",
    "        for i, index in enumerate(idx_permutation):\n",
    "            input_tensor, target_tensor = train_dl.tensorsFromPos(index)\n",
    "\n",
    "            loss, output_sentence = s2s.train(input_tensor, target_tensor)\n",
    "            score_printer.update(input_tensor, target_tensor, output_sentence, loss)\n",
    "            \n",
    "            if (i + 1) % print_every == 0:\n",
    "                score_printer.printAvg(print_every)\n",
    "        \n",
    "        score_printer.printAvg(len(train_dl))\n",
    "        val_avg_score = validate()\n",
    "        train_avg_score = score_printer.get_avg_score()\n",
    "        print(f\"Val_avg_score: {val_avg_score}, train_avg_score: {train_avg_score}\")\n",
    "        score_printer.endEpoch(epoch)\n",
    "\n",
    "        #torch.save(s2s.state_dict(),\"./state_dict.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(n = None):\n",
    "    score_printer = ScorePrinter(\"Validation\", [('NLL', loss_metric),('Perplexity', perplexity)])\n",
    "    n = n or len(val_dl)\n",
    "    idx_permutation_val = np.random.permutation(len(val_dl))[:n]\n",
    "    for j, val_index in enumerate(idx_permutation_val):\n",
    "        input_tensor_val, target_tensor_val = val_dl.tensorsFromPos(val_index)\n",
    "        loss, output_sentence = s2s.evaluate(input_tensor_val, target_tensor_val)\n",
    "        score_printer.update(input_tensor_val, target_tensor_val, output_sentence, loss)\n",
    "    score_printer.printAvg(showCount = False)\n",
    "    return score_printer.get_avg_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 started\n",
      " \n",
      "[Training] 10 examples /  NLL: 10.53 Perplexity: 37867.51  \n",
      "[Training] 20 examples /  NLL: 10.05 Perplexity: 27387.87  \n",
      "[Training] 30 examples /  NLL: 9.85 Perplexity: 23679.49  \n",
      "[Training] 40 examples /  NLL: 9.33 Perplexity: 19831.19  \n",
      "[Training] 50 examples /  NLL: 9.10 Perplexity: 16782.59 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d6f87dc558a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-06c5eed0a84f>\u001b[0m in \u001b[0;36mtrain_epochs\u001b[1;34m(epochs, print_every)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorsFromPos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms2s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mscore_printer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\NeuralMachineTranslation\\seq2seq.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_tensor, target_tensor)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicolas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nicolas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_epochs(2, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def print_validation(position):\n",
    "    input_sentence = val_dl.sentenceFromTensor('de', val_dl.tensorsFromPos(position)[0])\n",
    "    display(Markdown('**Eingabe**'))\n",
    "    display(Markdown(' '.join(input_sentence)))\n",
    "    prediction = s2s.predict(val_dl.tensorsFromPos(position)[0])\n",
    "    output_sentence = val_dl.sentenceFromTensor('en', prediction[0])\n",
    "    display(Markdown('**Ausgabe**'))\n",
    "    display(Markdown(' '.join(output_sentence)))\n",
    "    attentions = torch.stack([tensor.squeeze() for tensor in prediction[2]])\n",
    "    attentions = attentions.numpy()[:len(output_sentence)-1,:len(input_sentence)]\n",
    "    display(Markdown('**Attention**'))\n",
    "    show_attention(input_sentence, output_sentence, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def show_attention(input_sentence, output_sentence, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(16, 14), dpi= 80)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(input_sentence)))\n",
    "    ax.set_xticklabels(input_sentence, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(output_sentence[1:])))\n",
    "    ax.set_yticklabels(output_sentence[1:]) # ignore SOS Token\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
